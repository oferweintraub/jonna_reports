{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install required packages\n",
    "%pip install pymongo python-dotenv pandas seaborn matplotlib ipython boto3 anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MongoDB\n",
      "Users to analyze: ['ptr_dvd', 'SagiBarmak', 'KoheletForum']\n",
      "\n",
      "Analysis Configuration:\n",
      "Pre-war period: 2023-07-09 00:00:00 to 2023-10-07 00:00:00\n",
      "Post-war period: 2024-10-01 00:00:00 to 2024-12-30 00:00:00\n",
      "Days analyzed per period: 90\n",
      "Model: anthropic.claude-3-haiku-20240307-v1:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from data_extractor import MongoDBExtractor\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize MongoDB extractor\n",
    "extractor = MongoDBExtractor()\n",
    "extractor.connect()\n",
    "\n",
    "# 1. Selected users for analysis (subset of 3 from config.py)\n",
    "test_users = [\n",
    "    'ptr_dvd',      # Active Kohelet Forum member\n",
    "    'SagiBarmak',   # Prominent voice\n",
    "    'KoheletForum'  # Official account\n",
    "]\n",
    "print(\"Users to analyze:\", test_users)\n",
    "\n",
    "# 2. Define analysis periods\n",
    "pre_war_end = '2023-10-07'    # Day before the war\n",
    "post_war_start = '2024-12-30'  # War start date\n",
    "days_back = 90                 # Days to analyze for each period\n",
    "\n",
    "# Create timestamp objects for reference\n",
    "pre_war_end_date = datetime.strptime(pre_war_end, '%Y-%m-%d')\n",
    "post_war_start_date = datetime.strptime(post_war_start, '%Y-%m-%d')\n",
    "\n",
    "# 3. Additional parameters\n",
    "MODEL_NAME = \"anthropic.claude-3-haiku-20240307-v1:0\"  # Current model from analyzer.py\n",
    "\n",
    "# Directory structure for data organization\n",
    "DATA_DIRS = {\n",
    "    'raw': os.path.join('data', 'raw'),\n",
    "    'pre_war': os.path.join('data', 'raw', 'pre_war'),\n",
    "    'post_war': os.path.join('data', 'raw', 'post_war'),\n",
    "    'analysis': os.path.join('data', 'analysis'),\n",
    "    'cleaned': os.path.join('data', 'cleaned')\n",
    "}\n",
    "\n",
    "# Create necessary directories\n",
    "for dir_path in DATA_DIRS.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(f\"\\nAnalysis Configuration:\")\n",
    "print(f\"Pre-war period: {pre_war_end_date - timedelta(days=days_back)} to {pre_war_end_date}\")\n",
    "print(f\"Post-war period: {post_war_start_date - timedelta(days=days_back)} to {post_war_start_date}\")\n",
    "print(f\"Days analyzed per period: {days_back}\")\n",
    "print(f\"Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching and cleaning data for both periods...\n",
      "\n",
      "Processing pre-war period data...\n",
      "Successfully connected to MongoDB\n",
      "Fetching tweets from 2023-07-09 00:00:00 to 2023-10-07 23:59:59\n",
      "Using timestamps from 1688850000 to 1696712399\n",
      "Fetched 190 tweets for ptr_dvd\n",
      "Fetched 161 tweets for SagiBarmak\n",
      "Fetched 79 tweets for KoheletForum\n",
      "Saved raw data to: data\\raw\\pre_war\\tweets_pre_war_20250112_113046.csv\n",
      "\n",
      "Cleaning tweets...\n",
      "- Removing URLs\n",
      "- Removing @mentions\n",
      "- Filtering tweets with less than 7 words\n",
      "\n",
      "Tweet counts before and after cleaning:\n",
      "----------------------------------------------------------------------\n",
      "KoheletForum         - original:   79, cleaned:   70 (removed:    9,   11.4%)\n",
      "SagiBarmak           - original:  161, cleaned:  118 (removed:   43,   26.7%)\n",
      "ptr_dvd              - original:  190, cleaned:  142 (removed:   48,   25.3%)\n",
      "----------------------------------------------------------------------\n",
      "Total tweets - original: 430, after cleaning: 330\n",
      "Total removed: 100 (23.3%)\n",
      "\n",
      "Saved cleaned tweets to: data\\cleaned\\pre_war\\cleaned_pre_war_20250112_113046.csv\n",
      "Pre-war tweets after cleaning: 330\n",
      "\n",
      "Processing post-war period data...\n",
      "Successfully connected to MongoDB\n",
      "Fetching tweets from 2024-10-01 00:00:00 to 2024-12-30 23:59:59\n",
      "Using timestamps from 1727730000 to 1735595999\n",
      "Fetched 553 tweets for ptr_dvd\n",
      "Fetched 730 tweets for SagiBarmak\n",
      "Fetched 121 tweets for KoheletForum\n",
      "Saved raw data to: data\\raw\\post_war\\tweets_post_war_20250112_113047.csv\n",
      "\n",
      "Cleaning tweets...\n",
      "- Removing URLs\n",
      "- Removing @mentions\n",
      "- Filtering tweets with less than 7 words\n",
      "\n",
      "Tweet counts before and after cleaning:\n",
      "----------------------------------------------------------------------\n",
      "KoheletForum         - original:  121, cleaned:   95 (removed:   26,   21.5%)\n",
      "SagiBarmak           - original:  730, cleaned:  567 (removed:  163,   22.3%)\n",
      "ptr_dvd              - original:  553, cleaned:  410 (removed:  143,   25.9%)\n",
      "----------------------------------------------------------------------\n",
      "Total tweets - original: 1404, after cleaning: 1072\n",
      "Total removed: 332 (23.6%)\n",
      "\n",
      "Saved cleaned tweets to: data\\cleaned\\post_war\\cleaned_post_war_20250112_113047.csv\n",
      "Post-war tweets after cleaning: 1072\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Fetch and Clean Data\n",
    "from tweet_cleaner import TweetCleaner\n",
    "print(\"\\nFetching and cleaning data for both periods...\")\n",
    "\n",
    "# Initialize tweet cleaner with all parameters enabled\n",
    "cleaner = TweetCleaner(min_words=7, remove_mentions=True, remove_urls=True)\n",
    "\n",
    "# Process pre-war data\n",
    "print(\"\\nProcessing pre-war period data...\")\n",
    "pre_war_df = extractor.extract_tweets_by_date_range(\n",
    "    reference_date=pre_war_end,\n",
    "    days_back=days_back,\n",
    "    usernames=test_users,\n",
    "    period_label='pre_war'\n",
    ")\n",
    "pre_war_cleaned = cleaner.clean_tweets(pre_war_df, period_label='pre_war')\n",
    "print(f\"Pre-war tweets after cleaning: {len(pre_war_cleaned)}\")\n",
    "\n",
    "# Process post-war data\n",
    "print(\"\\nProcessing post-war period data...\")\n",
    "post_war_df = extractor.extract_tweets_by_date_range(\n",
    "    reference_date=post_war_start,\n",
    "    days_back=days_back,\n",
    "    usernames=test_users,\n",
    "    period_label='post_war'\n",
    ")\n",
    "post_war_cleaned = cleaner.clean_tweets(post_war_df, period_label='post_war')\n",
    "print(f\"Post-war tweets after cleaning: {len(post_war_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do users analysis (including merge analysis per user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing individual users for both periods...\n",
      "\n",
      "Analyzing pre-war period...\n",
      "\n",
      "Analyzing tweets for @ptr_dvd\n",
      "Total tweets: 142\n",
      "Number of batches: 3\n",
      "✓ Batch 1/3 completed\n",
      "✓ Batch 2/3 completed\n",
      "✓ Batch 3/3 completed\n",
      "\n",
      "Completed analysis for @ptr_dvd: 3 batches processed\n",
      "\n",
      "Analyzing tweets for @SagiBarmak\n",
      "Total tweets: 118\n",
      "Number of batches: 3\n",
      "✓ Batch 1/3 completed\n",
      "✓ Batch 2/3 completed\n",
      "✓ Batch 3/3 completed\n",
      "\n",
      "Completed analysis for @SagiBarmak: 3 batches processed\n",
      "\n",
      "Analyzing tweets for @KoheletForum\n",
      "Total tweets: 70\n",
      "Number of batches: 2\n",
      "✓ Batch 1/2 completed\n",
      "✓ Batch 2/2 completed\n",
      "\n",
      "Completed analysis for @KoheletForum: 2 batches processed\n",
      "\n",
      "Merging analyses for @KoheletForum\n",
      "Total batches to analyze: 2\n",
      "\n",
      "Merging analyses for @SagiBarmak\n",
      "Total batches to analyze: 3\n",
      "\n",
      "Merging analyses for @ptr_dvd\n",
      "Total batches to analyze: 3\n",
      "\n",
      "Saved merged analysis to: data\\analysis\\pre_war\\merged_analysis_pre_war_20250112_113226.csv\n",
      "Completed pre-war analysis for 3 users\n",
      "\n",
      "Analyzing post-war period...\n",
      "\n",
      "Analyzing tweets for @ptr_dvd\n",
      "Total tweets: 410\n",
      "Number of batches: 9\n",
      "✓ Batch 1/9 completed\n",
      "✓ Batch 2/9 completed\n",
      "✓ Batch 3/9 completed\n",
      "✓ Batch 4/9 completed\n",
      "✓ Batch 5/9 completed\n",
      "✓ Batch 6/9 completed\n",
      "✓ Batch 7/9 completed\n",
      "✓ Batch 8/9 completed\n",
      "✓ Batch 9/9 completed\n",
      "\n",
      "Completed analysis for @ptr_dvd: 9 batches processed\n",
      "\n",
      "Analyzing tweets for @SagiBarmak\n",
      "Total tweets: 567\n",
      "Number of batches: 12\n",
      "✓ Batch 1/12 completed\n",
      "✓ Batch 2/12 completed\n",
      "✓ Batch 3/12 completed\n",
      "✓ Batch 4/12 completed\n",
      "✓ Batch 5/12 completed\n",
      "✓ Batch 6/12 completed\n",
      "✓ Batch 7/12 completed\n",
      "✓ Batch 8/12 completed\n",
      "✓ Batch 9/12 completed\n",
      "✓ Batch 10/12 completed\n",
      "✓ Batch 11/12 completed\n",
      "✓ Batch 12/12 completed\n",
      "\n",
      "Completed analysis for @SagiBarmak: 12 batches processed\n",
      "\n",
      "Analyzing tweets for @KoheletForum\n",
      "Total tweets: 95\n",
      "Number of batches: 2\n",
      "✓ Batch 1/2 completed\n",
      "✓ Batch 2/2 completed\n",
      "\n",
      "Completed analysis for @KoheletForum: 2 batches processed\n",
      "\n",
      "Merging analyses for @KoheletForum\n",
      "Total batches to analyze: 2\n",
      "\n",
      "Merging analyses for @SagiBarmak\n",
      "Total batches to analyze: 12\n",
      "\n",
      "Merging analyses for @ptr_dvd\n",
      "Total batches to analyze: 9\n",
      "\n",
      "Saved merged analysis to: data\\analysis\\post_war\\merged_analysis_post_war_20250112_113442.csv\n",
      "Completed post-war analysis for 3 users\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Analyze Individual Users\n",
    "from analyzer import TweetAnalyzer\n",
    "print(\"\\nAnalyzing individual users for both periods...\")\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = TweetAnalyzer(batch_size=50, max_retries=3)\n",
    "\n",
    "# Process pre-war period\n",
    "print(\"\\nAnalyzing pre-war period...\")\n",
    "pre_war_analyses = []\n",
    "for username in pre_war_cleaned['author_username'].unique():\n",
    "    user_tweets = pre_war_cleaned[pre_war_cleaned['author_username'] == username].to_dict('records')\n",
    "    analysis = analyzer.analyze_user_tweets(username, user_tweets)\n",
    "    pre_war_analyses.append(analysis)\n",
    "pre_war_merged = analyzer.merge_user_analyses(pd.concat(pre_war_analyses), period_label='pre_war')\n",
    "print(f\"Completed pre-war analysis for {len(pre_war_merged)} users\")\n",
    "\n",
    "# Process post-war period\n",
    "print(\"\\nAnalyzing post-war period...\")\n",
    "post_war_analyses = []\n",
    "for username in post_war_cleaned['author_username'].unique():\n",
    "    user_tweets = post_war_cleaned[post_war_cleaned['author_username'] == username].to_dict('records')\n",
    "    analysis = analyzer.analyze_user_tweets(username, user_tweets)\n",
    "    post_war_analyses.append(analysis)\n",
    "post_war_merged = analyzer.merge_user_analyses(pd.concat(post_war_analyses), period_label='post_war')\n",
    "print(f\"Completed post-war analysis for {len(post_war_merged)} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhanced analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Analyze Individual Users\n",
    "from analyzer_enhanced import EnhancedTweetAnalyzer\n",
    "print(\"\\nAnalyzing individual users for both periods...\")\n",
    "\n",
    "# Initialize enhanced analyzer\n",
    "analyzer = EnhancedTweetAnalyzer(batch_size=50, max_retries=3)\n",
    "\n",
    "# Process pre-war period\n",
    "print(\"\\nAnalyzing pre-war period...\")\n",
    "pre_war_analyses = []\n",
    "for username in pre_war_cleaned['author_username'].unique():\n",
    "    user_tweets = pre_war_cleaned[pre_war_cleaned['author_username'] == username].to_dict('records')\n",
    "    analysis = analyzer.analyze_user_tweets(username, user_tweets)\n",
    "    pre_war_analyses.append(analysis)\n",
    "pre_war_merged = analyzer.merge_user_analyses_enhanced(pd.concat(pre_war_analyses), period_label='pre_war')\n",
    "print(f\"Completed pre-war analysis for {len(pre_war_merged)} users\")\n",
    "\n",
    "# Process post-war period\n",
    "print(\"\\nAnalyzing post-war period...\")\n",
    "post_war_analyses = []\n",
    "for username in post_war_cleaned['author_username'].unique():\n",
    "    user_tweets = post_war_cleaned[post_war_cleaned['author_username'] == username].to_dict('records')\n",
    "    analysis = analyzer.analyze_user_tweets(username, user_tweets)\n",
    "    post_war_analyses.append(analysis)\n",
    "post_war_merged = analyzer.merge_user_analyses_enhanced(pd.concat(post_war_analyses), period_label='post_war')\n",
    "print(f\"Completed post-war analysis for {len(post_war_merged)} users\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jonna_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
